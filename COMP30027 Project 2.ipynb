{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict as dd\n",
    "from collections import Counter\n",
    "from random import randint\n",
    "from copy import deepcopy, copy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = '2019S1-proj2-data/dev-raw.tsv'\n",
    "\n",
    "def readData(filename):\n",
    "    with open(filename,'r') as tsv:\n",
    "        data = [line.strip().split('\\t') for line in tsv]\n",
    "\n",
    "    dataDict = {\"labels\": [], \"tweets\": [], \"Id\": []}\n",
    "    for line in data:\n",
    "        dataDict[\"Id\"].append(line[0])\n",
    "        dataDict[\"labels\"].append(line[1])\n",
    "        dataDict[\"tweets\"].append(line[2])\n",
    "    \n",
    "    return dataDict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \n",
    "    preprocessedData = []\n",
    "    \n",
    "    for e in data:\n",
    "        ePreprocessed = e.strip().split(\" \")\n",
    "        for term in e:\n",
    "            if term[0] == \"@\":\n",
    "                ePreprocessed += [term]*3\n",
    "        ePreprocessed = [f.lower() for f in ePreprocessed]\n",
    "        preprocessedData.append(ePreprocessed)\n",
    "    \n",
    "    return preprocessedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLH(data, labels):\n",
    "    \n",
    "    assert(len(data)==len(labels))\n",
    "    \n",
    "    termLocFrequencies = dd(Counter)\n",
    "    totalTermsLoc = Counter()\n",
    "    totalTermsAll = 0\n",
    "    termCounts = Counter()\n",
    "    \n",
    "    for i in range(0, len(data)):\n",
    "        terms = data[i]\n",
    "        loc = labels[i]\n",
    "        \n",
    "        for term in terms:\n",
    "            termLocFrequencies[\"\".join(filter(str.isalnum,term))][loc] += 1\n",
    "            totalTermsLoc[loc] += 1\n",
    "            totalTermsAll += 1\n",
    "            termCounts[term] += 1\n",
    "    \n",
    "    wlh = {}\n",
    "    wlhCountRestricted = {}\n",
    "    \n",
    "    for term in termLocFrequencies.keys():\n",
    "        \n",
    "        denom = sum(termLocFrequencies[term].values())/totalTermsAll\n",
    "        num = max([(termLocFrequencies[term][city]/totalTermsLoc[city], city) if totalTermsLoc[city] != 0 else (0, city) for city in (\"Melbourne\", \"Sydney\", \"Perth\", \"Brisbane\")])\n",
    "        \n",
    "        wlh[term] = num[0]/denom\n",
    "        if termCounts[term] > 4:\n",
    "            wlhCountRestricted[term] = num[0]/denom\n",
    "    \n",
    "    return wlh, wlhCountRestricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelect(wlh):\n",
    "    \n",
    "#     wlhSorted = sorted([(y,x) for (x,y) in wlh.items()], reverse=True)\n",
    "#     wlhBest = wlhSorted[0:int(len(wlh.keys())/3)]\n",
    "    wlhBest = [(y,x) for (x,y) in wlh.items() if y>=2.5]\n",
    "    wlhBestSet = set([a[1] for a in wlhBest])\n",
    "    indexes = {}\n",
    "    for i in range(0, len(wlhBest)):\n",
    "        indexes[wlhBest[i][1]] = i\n",
    "    \n",
    "    return wlhBestSet, indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, wlhBestSet):\n",
    "    finalData = []\n",
    "    \n",
    "    for e in data:\n",
    "        encoded = np.zeros(len(wlhBestSet))\n",
    "        for term in e:\n",
    "            if term in wlhBestSet:\n",
    "                encoded[indexes[term]] += 1\n",
    "                \n",
    "        finalData.append(encoded)\n",
    "    \n",
    "    return finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def getLocation(tweet):\n",
    "    m = re.search(\"@\\s(([a-zA-Z]+(\\s|,|\\))))+\", tweet)\n",
    "    if m:\n",
    "        loc = re.sub(\"([^\\w]|[\\d_])+\", \" \",  m.group(0).lower()).strip()\n",
    "        return loc\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLocationDict(data, labels):\n",
    "    assert(len(data) == len(labels))\n",
    "    locDict = {}\n",
    "    for i in range(0, len(data)):\n",
    "        locTag = getLocation(data[i])\n",
    "        if locTag:\n",
    "            locDict[locTag] = labels[i]\n",
    "    return locDict           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUsers(tweet):\n",
    "    return re.findall(\"(@[\\w\\d]+)\", tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweetAugment(data, labels):\n",
    "    \n",
    "    assert(len(data)==len(labels))\n",
    "    \n",
    "    augmentedDataDict = {\"tweets\": [], \"labels\": copy(labels)}\n",
    "    \n",
    "    for tweet in data:\n",
    "        new = copy(tweet)\n",
    "        \n",
    "        location = getLocation(tweet)\n",
    "        users = getUsers(tweet)\n",
    "        \n",
    "        for user in users:\n",
    "            new += (\" \" + user)*4\n",
    "        \n",
    "        if location:\n",
    "            new += (\" \" + location)*4\n",
    "        \n",
    "        augmentedDataDict[\"tweets\"].append(new)\n",
    "\n",
    "    return augmentedDataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readyDataSet(filename, wlhSet=None, augment=False):\n",
    "    dataDict = readData(filename)\n",
    "    originalDict = deepcopy(dataDict)\n",
    "    locDict = buildLocationDict(dataDict[\"tweets\"], dataDict[\"labels\"])\n",
    "    \n",
    "    if augment:\n",
    "        dataDict = tweetAugment(dataDict[\"tweets\"], dataDict[\"labels\"])\n",
    "    dataDict[\"tweets\"] = preprocess(dataDict[\"tweets\"])\n",
    "    \n",
    "    if not wlhSet:\n",
    "        wlh = WLH(dataDict[\"tweets\"], dataDict[\"labels\"])\n",
    "        wlhBestSet, indexes = featureSelect(wlh)\n",
    "    else:\n",
    "        wlhBestSet = wlhSet\n",
    "        wlh = None\n",
    "\n",
    "    for i in range(0, len(dataDict[\"tweets\"])):\n",
    "        tweet = dataDict[\"tweets\"][i]\n",
    "        new = []\n",
    "        for term in tweet:\n",
    "            if \"\".join(filter(str.isalnum, term.lower())) in wlhBestSet:\n",
    "                new.append(\"\".join(filter(str.isalnum, term.lower())))\n",
    "\n",
    "        dataDict[\"tweets\"][i] = \" \".join(new)\n",
    "        \n",
    "    return dataDict, wlhBestSet, locDict, originalDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our principle model: Naive Bayes and data enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = readData('2019S1-proj2-data/train-raw.tsv')\n",
    "augmentedDataDict = tweetAugment(dataDict[\"tweets\"], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataDict = readData('2019S1-proj2-data/dev-raw.tsv')\n",
    "augmentedDevDataDict = tweetAugment(devDataDict[\"tweets\"], devDataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlh, wlhCountRestricted = WLH([x.split(\" \") for x in dataDict[\"tweets\"]], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "topWlh = [x.lower() for (x,y) in wlhCountRestricted.items() if y > 2.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentTestData(data, topWlh):\n",
    "    new = []\n",
    "    for i in range(0, len(data)):\n",
    "        tweet = data[i]\n",
    "        newTweet = copy(data[i])\n",
    "        for j in topWlh:\n",
    "            if j in tweet.lower():\n",
    "                newTweet += (\" \" + j)\n",
    "        new.append(newTweet)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with dev set augmentation: 0.34939436166791726\n",
      "accuracy without dev set augmentation: 0.3484296280415907\n",
      "accuracy with new dev set augmentation: 0.3624986600921857\n"
     ]
    }
   ],
   "source": [
    "geotaggerNBMultinomial5 = Pipeline([('count', CountVectorizer()), ('classifier', MultinomialNB()),])\n",
    "geotaggerNBMultinomial5.fit(augmentTestData(augmentedDataDict[\"tweets\"], topWlh), augmentedDataDict[\"labels\"])\n",
    "\n",
    "print(\"accuracy with dev set augmentation: {}\".format(geotaggerNBMultinomial5.score(augmentedDevDataDict[\"tweets\"], augmentedDevDataDict[\"labels\"])))\n",
    "print(\"accuracy without dev set augmentation: {}\".format(geotaggerNBMultinomial5.score(devDataDict[\"tweets\"], devDataDict[\"labels\"])))\n",
    "\n",
    "print(\"accuracy with new dev set augmentation: {}\".format(geotaggerNBMultinomial5.score(augmentTestData(augmentedDevDataDict[\"tweets\"], topWlh), devDataDict[\"labels\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = geotaggerNBMultinomial5.predict(augmentTestData(augmentedTestDataDict[\"tweets\"], topWlhComb))\n",
    "output = pd.DataFrame({\"Id\": augmentedTestDataDict[\"Id\"], \"Class\": predictions})\n",
    "output.to_csv(\"primary-model.tsv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"@KellyFrye @girlscouts when life gives you cookies... \\\\ud83c\\\\udf6a\\\\ud83c\\\\udf6a\\\\ud83c\\\\udf6a\\\\ud83c\\\\udf6a\"',\n",
       " '\"@noviarezki naaaah itu dia hahaha\"',\n",
       " '\"#LouisWhyAreYouAnEGG  bro whyyy??? http://t.co/i4AhvJHPbj\"',\n",
       " '\"\\\\\"Twt: 20 creepy kids\\' drawings that will haunt your nightmares... http://t.co/4WUa8JHAdy http://t.co/CwXSFRtuDL\\\\\"\"',\n",
       " '\"Temp: 21.5\\\\u00b0C. Wind:11.5km/h. Pressure: 1019.7 hPa, Falling slowly. Humidity 55%. Rain Today 0.0mm. #HammondPark #Weather\"',\n",
       " '\"In situ... http://t.co/X96m46OuOL\"',\n",
       " '\"An out-of-sessions court hearing will be held very shortly in relation to stabbing murder of 17 year old Masa Vukotic.@TheTodayShow\"',\n",
       " '\"T-MINUS 24 HOURS #igsg #\\\\u8981graduate\\\\u54af @ Surfers Paradise, Gold Coast http://t.co/q1Hs8MOOc1\"',\n",
       " '\"S P O T L I G H T \\\\ud83d\\\\udd06 we feel it\\'s a public service announcement to let you all know that the amazing\\\\u2026 https://t.co/i2PrPPqXXH\"',\n",
       " '\"@LTUcareers Trying to sign up for Career Ready Courses. I get redirected to LMS with error message that I cannot enrol myself in the course.\"',\n",
       " '\"AU : Preventing suicide \\\\u2013 \\\\u201cno greater legacy\\\\u201d \\\\u00a0- The majority of suicides are believed to have stressors arising\\\\u2026 http://t.co/nhonWRLRJi\"',\n",
       " '\"\\\\u201c@MargsLD: One virginal Scottish hunk \\\\u2714\"',\n",
       " '\"@TydeLevi obvssssss \\\\ud83d\\\\udc9e\"',\n",
       " '\"night all enjoyable evening again.\"',\n",
       " '\"@xerexex please share our piece on our favourite #lifescience #books here http://t.co/tqfUTq6C4e #biodetectives thank you!\"',\n",
       " '\"The dorkiest dorks that ever lived\\\\ud83d\\\\ude18\\\\ud83d\\\\ude0d http://t.co/Zqyf43K8m8\"',\n",
       " '\"@dale_roots It\\'s confusing for me, twice I\\'ve expected play to be stopped because I heard the whistle and it\\'s just been guy in the crowd\"',\n",
       " '\"@AndyHowe_statto @BrentonSpeed @behindthegamepd certainly not, but slowing it down I think he was fractionally off\"',\n",
       " '\"I have finished #HouseofCards Season 3. Let\\'s all start a group DM and talk every detail of it to death.\"',\n",
       " '\"first shift back at maccas in 2 months. gets put on fries for 4 hours. #imlovinit #jk #saveme #fml\"',\n",
       " '\"@HuffingtonPost wish I was there to help #STOPKillingDolphins #helpsavethewales\"',\n",
       " '\"Shenwari afganistan v Bangladesh #CastrolCatches\"',\n",
       " '\"@GlennJ1989 yeah good point, people in this community can be pretty shifty. At least -you\\'re- straight up about being a troll, lel.\"',\n",
       " '\"@simpppqueen no offense to you but I can wait this place is amazing \\\\ud83d\\\\ude0d\"',\n",
       " '\"\\\\u201c@autocorrects: who wants to join my club http://t.co/81zxvUQNKA\\\\u201d \\\\ud83d\\\\ude4b\"',\n",
       " '\"#Breakfast Best #Hangovercure. Head down, arse up and work it off.No detriment to Catholics, Islam or other sects.\"',\n",
       " '\"Oh Jamie you stubborn hard headed Scot.  #OutlanderRewind\"',\n",
       " '\"Literally getting anxiety bc I know I won\\'t meet cam :(\"',\n",
       " '\"Em disco moment Aha lol had a great time tho xoxo \\\\ud83d\\\\udc90\\\\ud83d\\\\udc90\\\\ud83d\\\\ude0d\\\\ud83d\\\\ude0d\\\\ud83d\\\\udc97\\\\ud83d\\\\udc97\\\\ud83d\\\\udc95\\\\ud83d\\\\udc4c\\\\ud83d\\\\udc4c\\\\ud83d\\\\udc4c http://t.co/cU0mayggRe\"',\n",
       " '\"@EIectricEden Josh\"',\n",
       " '\"@NBTConor aesthetics/10\"',\n",
       " '\"Something for all 90s kids http://t.co/uBfTVtafIx\"',\n",
       " '\"My boyfriend is fucking adorable okay\"',\n",
       " '\"QUESTION ARE THE ORANGE POLICE SELLING HEAVY DRUGS IN OUR TOWN ? IF SO WHO ARE THE SELECTED DEALERS WORKING FOR THEM ? WHERE IT COMING FROM?\"',\n",
       " '\"Pleaseeee Harry! \\\\ud83d\\\\ude4c @Harry_Styles #harrystyles http://t.co/5Iax0mXpPn\"',\n",
       " '\"Kate Carnell would never work on a weekend or accept a pay cut. But she expects others to do the same #hypocrite #doublestandards #abc730\"',\n",
       " '\"@CharmedDaily when if there is remaking of the showI just love it\"',\n",
       " '\"@MetzAUS bro I\\'m 15 minutes late cause of this bitch\"',\n",
       " '\"@ArianaGrande I love u so so much \\\\ud83d\\\\udc96\"',\n",
       " '\"@gopack7777 ..cheers from Australia..I hope you support 49er\\'s..lol\"',\n",
       " '\"@s4mmi3x @Thekeifergirl81 @SpiritWhiteEagl @ConsciousKatOK @ConnieFortune @monicasloves So cool \\\\u2764\\\\ufe0f\\\\ud83d\\\\ude47\"',\n",
       " '\"Broad tells Smith if he was a decent bloke that he should\\'ve walked on that one #CWC15 #AUSvENG\"',\n",
       " '\"Appreciation post for gears! Unfortunately got unlucky today w/ broken cable and raced the final 30km with two gear options #OceaniaChamps\"',\n",
       " '\"@tyleroakley babe its 7 days until my birthday (in Sydney 6 days) its on a Tuesday and can I be Person Of The Week? http://t.co/2FRyhJKxdf\"',\n",
       " '\"@larissawaters @stunuts1948 utterly unfit to be prime minister..\"',\n",
       " '\"@Optus hey could you tell your support team to pick up phones because I pay for internet and right now I am paying for an orange light!\"',\n",
       " '\"#bestbfever #hbdtome #21 #feast #breakfast http://t.co/BA0WrtvWnd\"',\n",
       " '\"@paulaaf08 achei ofensivo\"',\n",
       " '\"@krisbrooksmusic You better #bootleg.\"',\n",
       " '\"\\\\u2729\"',\n",
       " '\"slightly old #Tedtalk (based on the Lance Armstrong reference) but something worth watching.  Great for #managers! https://t.co/dp6Iblzw0R\"',\n",
       " '\"Met a nice girl on tinder, think there\\'s space for a relationship + cuddling http://t.co/C0c1KSSxth\"',\n",
       " '\"@AidenRemy Kogo?\"',\n",
       " '\"@pureNZweddings @GlassOfBubbly @WMhappyhour cool clip. #dutz cap is gr8 to wear!\"',\n",
       " '\"\\\\\"Big fucking metadata\\\\\" #bigdata #hadoop http://t.co/sXx61dZL74\"',\n",
       " '\"\\\\u0645\\\\u0639 \\\\u0627\\\\u0646\\\\u064a \\\\u0636\\\\u062f \\\\u0628\\\\u0639\\\\u0636 \\\\u0627\\\\u0641\\\\u0643\\\\u0627\\\\u0631 \\\\u0644\\\\u062c\\\\u064a\\\\u0646 \\\\u0627\\\\u0644\\\\u0627 \\\\u0627\\\\u0646\\\\u0647\\\\u0627 \\\\u0643\\\\u0641\\\\u0648 \\\\u0648\\\\u0637\\\\u0627\\\\u0644\\\\u0628\\\\u062a \\\\u0628\\\\u062d\\\\u0642 \\\\u0645\\\\u0646 \\\\u062d\\\\u0642\\\\u0648\\\\u0642\\\\u0647\\\\u0627 \\\\u0648\\\\u062a\\\\u062c\\\\u064a\\\\u0643 \\\\u0648\\\\u062d\\\\u062f\\\\u0647 \\\\u0647\\\\u0628\\\\u0644\\\\u0627 \\\\u0645\\\\u0633\\\\u062a\\\\u0639\\\\u0628\\\\u062f\\\\u0647 \\\\u0645\\\\u0627\\\\u0644\\\\u0647\\\\u0627 \\\\u0631\\\\u0623\\\\u064a \\\\u062a\\\\u0642\\\\u0648\\\\u0644 \\\\u0639\\\\u0646\\\\u0647\\\\u0627 \\\\\"\\\\u062e\\\\u0631\\\\u064a\\\\u062c\\\\u0629 \\\\u0633\\\\u062c\\\\u0648\\\\u0646\\\\\" \\\\u064a\\\\u0646\\\\u0639\\\\u0646 \\\\u0627\\\\u0628\\\\u0648 \\\\u0641\\\\u0643\\\\u0631\\\\u0643\\\\u0645 \\\\u0627\\\\u0644\\\\u0645\\\\u062a\\\\u062d\\\\u062c\\\\u0631\\\\ud83d\\\\udc7d\\\\ud83d\\\\udd25\"',\n",
       " '\"@HaksunKim \\\\ud751 \\\\ud655\\\\uc2e4\\\\ud788...\"',\n",
       " '\"@3_D_Publishing can we understand something here I didn\\'t say anything about Race other then how the term RACE IN CONTEXT OF Jews is used\"',\n",
       " '\"#selfie @ Swanbourne Nudist Beach https://t.co/iAkDC67zqy\"',\n",
       " '\"@mooscolly haha no worries, there\\'ll be a soundcloud stream later too.\"',\n",
       " '\"@cblunt58 @hymnforrachel the pain itself, is it sharp when you move your knee or kind of a dull all the time with spikes?\"',\n",
       " '\"@VithiG exactly\"',\n",
       " '\"This is H E A V E N ! \\\\ud83d\\\\ude0d\\\\ud83d\\\\ude0d @ Koko Black Claremont Quater https://t.co/RtVa8h0ymg\"',\n",
       " '\"i have a lot to be thankful for, thank you lord\"',\n",
       " '\"I blogged about my fave booktubers last week ft @LaurenWhitehead @aeroplanegirl @peruseproject @rose_mannering go check it and them out \\\\ud83d\\\\udcda\"',\n",
       " '\"@COLRICHARDKEMP Yoicks...why do 500 people \\'favourite\\' your tweets even if you don\\'t say anything earth-shattering?\"',\n",
       " '\"\\\\\"@AlexisGZall: quote this with your favorite emoji if you\\'d like to be added to zallchat on http://t.co/vRwIFkaGgs this week! \\\\ud83c\\\\udf89\\\\ud83c\\\\udf89\\\\ud83c\\\\udf89\\\\\"\\\\ud83d\\\\udc80\\\\ud83d\\\\udca9\"',\n",
       " '\"@TeriPolo1   My first born child is off to university this week. He grew up so fast, such mixed emotions... http://t.co/nTcAnsu672\"',\n",
       " '\"3/3 that they were exhausting &amp; emotionally difficult to conduct defence case &amp; he would take a months holiday after each to recover.\"',\n",
       " '\"Pleaseeee Harry! \\\\ud83d\\\\ude4c @Harry_Styles http://t.co/F1iwT1T8Je\"',\n",
       " '\"The PG we need is @TyLawson3 or some other assist minded point guard with a threatening 3pt shot! @HoustonRockets #RedNation #HTOWN #Pursuit\"',\n",
       " '\"@brianfenton01 R u on The New One this year?\"',\n",
       " '\"Last night: Red wines and bourbons.Today: Berocca, coffees and H2O.\"',\n",
       " '\"@sophellenbyrne @adelaidefest Can\\'t wait to bump into you for literary hangs at #Adlww. No mad sleep-deprived rushing around. Just lit-bliss\"',\n",
       " '\"//YOUR// PET ANGEL\"',\n",
       " '\".@billshortenmp right. Attack dogs @TonyAbbottMHR &amp; Brandis @LiberalAus unsuited to be nation\\'s leader and AG. #IStandWithGillianTriggs\"',\n",
       " '\"Wish everyone retained their innocence #liam #tomasi #stripback #fifa #movienights #oldfashioned #fun\"',\n",
       " '\"@bythelakewithme\"',\n",
       " '\"@meadea @Bilbulbabe such a wonderful human being @DalaiLama and he just nails it every single time. Thanks for posting this!\"',\n",
       " '\"#DontDoYourDash because #EverybodyScoresEventually! http://t.co/z20evfTWy5\"',\n",
       " '\"quick interruption but thank u for 800 xx #vote5sos #KCA http://t.co/JjLoeOLem8\"',\n",
       " '\"@Optus Already been checked by @Optus a few times - There\\'s no cable running on my street.\"',\n",
       " '\"Last photo from this shoot. Be sure to check out my website for more photos. #vsco #vscocam #style\\\\u2026 https://t.co/WgPPJqg4hA\"',\n",
       " '\"REMEMBER WHEN LUKE WISHED ME A HAOOT BEIRDAY AND HW WAS LIKE \\\\\"happy birthday to a special girl\\\\\" STOP\"',\n",
       " '\"Only 13 hours? RT @Becca_Gilmour: I\\'m counting on @mrtravisburns to tweet a lot for the next 13 hours to keep me awake tonight! \\\\u263a\\\\ufe0f xx\"',\n",
       " '\"Why is it Bali Bombers who murdered 202 people including 88 Australians r in jail 4 life but 2 guys 9yrs in prison rehabilitated r executed\"',\n",
       " '\"Andrea #Pirlo Ungkap Ganjalan #Juventus Di Eropa - https://t.co/k3dZ5GKTT9 http://t.co/M7mniaisEZ\"',\n",
       " '\"@benshephard ben how can I watch online?\"',\n",
       " '\"@CampbellC_123 @colmburn poor nate dog. It must be gone to a bad place when even your throwing in the towel Campbell! #johnmagee #natedog\"',\n",
       " '\"I have the cutest dog \\\\ud83d\\\\ude4a\\\\ud83d\\\\ude48\\\\ud83d\\\\ude0d\"',\n",
       " '\"Looking for something to do on the long weekend...check this out http://t.co/t1ZSviGqUG http://t.co/6jAIE7tRW2\"',\n",
       " '\"53% of employees in the  #financialservices industry are #women in #Australia, says Loane.\"',\n",
       " '\"I\\'m at Vaucluse https://t.co/QjxnEewqOg\"',\n",
       " '\"Beau knows is too good.\"',\n",
       " '\"Don\\'t find elly sharp attractive anymore    :(\"',\n",
       " '\"@ANZ_AU Great initiative, love this\"',\n",
       " '\"MY MOTHER DOESNT UNDERSTAND\"',\n",
       " '\"Perso la #sim del telefono. Sostituita gratis con una nuova. 5 minuti, costo 0 e tutto il credito sopra. Fantastico! #optus #australia\"',\n",
       " '\"omg pls\"',\n",
       " '\"@CallAtkinson haha my eyes are fucked everyday mate, out on beers now and I\\'m up for work at 4:30am....\\\\ud83d\\\\ude34\\\\ud83d\\\\ude34\"']"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(geotaggerNBMultinomial5.predict(augmentTestData(augmentedDevDataDict[\"tweets\"], topWlh))).eq(pd.Series(devDataDict[\"labels\"]))\n",
    "[devDataDict[\"tweets\"][i] for i in a.index[a==False].tolist()[0:100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle submit code\n",
    "Combine test and dev sets for training for optimal ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlhComb, wlhCountRestrictedComb = WLH([x.split(\" \") for x in (dataDict[\"tweets\"]+devDataDict[\"tweets\"])], dataDict[\"labels\"]+devDataDict[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "topWlhComb = [x.lower() for (x,y) in wlhCountRestrictedComb.items() if y > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDictCombined = {\"tweets\": dataDict[\"tweets\"]+devDataDict[\"tweets\"], \"labels\": dataDict[\"labels\"]+devDataDict[\"labels\"]}\n",
    "dataDictCombinedAugmented = tweetAugment(dataDictCombined[\"tweets\"], dataDictCombined[\"labels\"])\n",
    "geotaggerNBMultinomial6 = Pipeline([(\"tfidf\", TfidfVectorizer()), ('classifier', MultinomialNB()),])\n",
    "geotaggerNBMultinomial6.fit(augmentTestData(dataDictCombinedAugmented[\"tweets\"], topWlhComb), dataDictCombined[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataDict = readData('2019S1-proj2-data/test-raw.tsv')\n",
    "augmentedTestDataDict = tweetAugment(testDataDict[\"tweets\"], testDataDict[\"labels\"])\n",
    "augmentedTestDataDict[\"Id\"] = testDataDict[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = geotaggerNBMultinomial6.predict(augmentTestData(augmentedTestDataDict[\"tweets\"], topWlhComb))\n",
    "output = pd.DataFrame({\"Id\": augmentedTestDataDict[\"Id\"], \"Class\": predictions})\n",
    "output.to_csv(\"kagglePredictionsNew.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataDict = readData('2019S1-proj2-data/train-raw.tsv')\n",
    "devDataDict = readData('2019S1-proj2-data/dev-raw.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34229285025190265"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselineClassifier = Pipeline([(\"count\", CountVectorizer()), ('classifier', MultinomialNB()),])\n",
    "baselineClassifier.fit(trainDataDict[\"tweets\"], trainDataDict[\"labels\"])\n",
    "baselineClassifier.score(devDataDict[\"tweets\"], devDataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfNB = Pipeline([(\"tfidf\", TfidfVectorizer()), ('classifier', MultinomialNB()),])\n",
    "tfidfNB.fit(trainDataDict[\"tweets\"], trainDataDict[\"labels\"])\n",
    "tfidfNB.score(devDataDict[\"tweets\"], devDataDict[\"labels\"])\n",
    "predictions = tfidfNB.predict(testDataDict[\"tweets\"])\n",
    "output = pd.DataFrame({\"Id\": augmentedTestDataDict[\"Id\"], \"Class\": predictions})\n",
    "output.to_csv(\"simple-model.tsv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3408189516561261"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfLinearSVC = Pipeline([(\"tfidf\", TfidfVectorizer()), ('classifier', LinearSVC()),])\n",
    "tfidfLinearSVC.fit(trainDataDict[\"tweets\"], trainDataDict[\"labels\"])\n",
    "tfidfLinearSVC.score(devDataDict[\"tweets\"], devDataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromehan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/jeromehan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33537892592989604"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfLogit = Pipeline([(\"tfidf\", TfidfVectorizer()), ('classifier', LogisticRegression()),])\n",
    "tfidfLogit.fit(trainDataDict[\"tweets\"], trainDataDict[\"labels\"])\n",
    "tfidfLogit.score(devDataDict[\"tweets\"], devDataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31262729124236255"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfDT = Pipeline([(\"tfidf\", TfidfVectorizer()), ('classifier', tree.DecisionTreeClassifier()),])\n",
    "tfidfDT.fit(trainDataDict[\"tweets\"], trainDataDict[\"labels\"])\n",
    "tfidfDT.score(devDataDict[\"tweets\"], devDataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Better preprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with new dev set augmentation: 0.33570050380533817\n"
     ]
    }
   ],
   "source": [
    "preprocessLinearSVC = Pipeline([('count', CountVectorizer()), ('classifier', LinearSVC(max_iter=2000)),])\n",
    "preprocessLinearSVC.fit(augmentTestData(augmentedDataDict[\"tweets\"], topWlh), augmentedDataDict[\"labels\"])\n",
    "\n",
    "print(\"accuracy with new dev set augmentation: {}\".format(preprocessLinearSVC.score(augmentTestData(augmentedDevDataDict[\"tweets\"], topWlh), devDataDict[\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromehan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/jeromehan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with new dev set augmentation: 0.3534408832672312\n"
     ]
    }
   ],
   "source": [
    "preprocessLogit = Pipeline([('count', CountVectorizer()), ('classifier', LogisticRegression()),])\n",
    "preprocessLogit.fit(augmentTestData(augmentedDataDict[\"tweets\"], topWlh), augmentedDataDict[\"labels\"])\n",
    "\n",
    "print(\"accuracy with new dev set augmentation: {}\".format(preprocessLogit.score(augmentTestData(augmentedDevDataDict[\"tweets\"], topWlh), devDataDict[\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with new dev set augmentation: 0.31905884875120594\n"
     ]
    }
   ],
   "source": [
    "preprocessDT = Pipeline([('count', CountVectorizer()), ('classifier', tree.DecisionTreeClassifier()),])\n",
    "preprocessDT.fit(augmentTestData(augmentedDataDict[\"tweets\"], topWlh), augmentedDataDict[\"labels\"])\n",
    "\n",
    "print(\"accuracy with new dev set augmentation: {}\".format(preprocessDT.score(augmentTestData(augmentedDevDataDict[\"tweets\"], topWlh), devDataDict[\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(dataDict):\n",
    "    finalOutput = []\n",
    "    \n",
    "    svcPredict = geotaggerLinearSVC.predict(dataDict[\"tweets\"])\n",
    "    nbPredict = geotaggerNBMultinomial.predict(dataDict[\"tweets\"])\n",
    "    logitPredict = geotaggerLogit.predict(dataDict[\"tweets\"])\n",
    "    rfPredict = geotaggerRF.predict(dataDict[\"tweets\"])\n",
    "    dtPredict = geotaggerDT.predict(dataDict[\"tweets\"])\n",
    "    \n",
    "    for i in range(0, len(dataDict[\"tweets\"])):\n",
    "        votes = Counter()\n",
    "        \n",
    "        votes[svcPredict[i]] += 1*0.75\n",
    "        votes[nbPredict[i]] += 1\n",
    "        votes[logitPredict[i]] += 1*0.55\n",
    "        votes[rfPredict[i]] += 1*0.4\n",
    "        votes[dtPredict[i]] += 1*0.3\n",
    "        \n",
    "        optimal = []\n",
    "        maxVotes = 0\n",
    "        \n",
    "        for label in votes.keys():\n",
    "            if votes[label] > maxVotes:\n",
    "                maxVotes = votes[label]\n",
    "                optimal = [label]\n",
    "            elif votes[label] == maxVotes:\n",
    "                optimal.append(label)\n",
    "        \n",
    "        if len(optimal) > 1:\n",
    "            finalOutput.append(optimal[randint(0,len(optimal)-1)])\n",
    "        else:\n",
    "            finalOutput.append(optimal[0])\n",
    "\n",
    "    return finalOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotaggerNBMultinomial = Pipeline([('tfidf', TfidfVectorizer()), ('classifier', MultinomialNB()),])\n",
    "geotaggerNBMultinomial.fit(dataDict[\"tweets\"], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotaggerLinearSVC = Pipeline([('tfidf', TfidfVectorizer()), ('classifier', LinearSVC()),])\n",
    "geotaggerLinearSVC.fit(dataDict[\"tweets\"], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromehan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/jeromehan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotaggerLogit = Pipeline([('tfidf', TfidfVectorizer()), ('classifier', LogisticRegression()),])\n",
    "geotaggerLogit.fit(dataDict[\"tweets\"], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromehan/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotaggerRF = Pipeline([('tfidf', TfidfVectorizer()), ('classifier', RandomForestClassifier()),])\n",
    "geotaggerRF.fit(dataDict[\"tweets\"], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))])"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotaggerDT = Pipeline([('tfidf', TfidfVectorizer()), ('classifier', tree.DecisionTreeClassifier()),])\n",
    "geotaggerDT.fit(dataDict[\"tweets\"], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vote accuracy: 0.2490352663736735\n"
     ]
    }
   ],
   "source": [
    "votePredictions = voting(devDataDict)\n",
    "print(\"vote accuracy: {}\".format(pd.Series(votePredictions).eq(pd.Series(dataDict[\"labels\"])).sum()/len(votePredictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = voting(testDataDict)\n",
    "output = pd.DataFrame({\"Id\": augmentedTestDataDict[\"Id\"], \"Class\": predictions})\n",
    "output.to_csv(\"voting-model.tsv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Kaggle stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLHNew(data, labels):\n",
    "    \n",
    "    assert(len(data)==len(labels))\n",
    "    \n",
    "    termLocFrequencies = dd(Counter)\n",
    "    totalTermsLoc = Counter()\n",
    "    totalTermsAll = 0\n",
    "    termCounts = Counter()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    for i in range(0, len(data)):\n",
    "        terms = data[i]\n",
    "        loc = labels[i]\n",
    "        \n",
    "        for term in terms:\n",
    "            termLocFrequencies[\"\".join(filter(str.isalnum,term))][loc] += 1\n",
    "            totalTermsLoc[loc] += 1\n",
    "            totalTermsAll += 1\n",
    "            termCounts[term] += 1\n",
    "            vocabulary.add(term.lower())\n",
    "    \n",
    "    wlh = {}\n",
    "    wlhCountRestricted = {}\n",
    "    \n",
    "    for term in termLocFrequencies.keys():\n",
    "        \n",
    "        denom = sum(termLocFrequencies[term].values())/totalTermsAll\n",
    "        num = max([(termLocFrequencies[term][city]/totalTermsLoc[city], city) if totalTermsLoc[city] != 0 else (0, city) for city in (\"Melbourne\", \"Sydney\", \"Perth\", \"Brisbane\")])\n",
    "        \n",
    "        wlh[term] = num[0]/denom\n",
    "        if termCounts[term] > 4:\n",
    "            wlhCountRestricted[term] = num[0]/denom\n",
    "    \n",
    "    return wlh, wlhCountRestricted, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = readData('2019S1-proj2-data/train-raw.tsv')\n",
    "augmentedDataDict = tweetAugment(dataDict[\"tweets\"], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataDict = readData('2019S1-proj2-data/dev-raw.tsv')\n",
    "augmentedDevDataDict = tweetAugment(devDataDict[\"tweets\"], devDataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlh, wlhCountRestricted, vocabulary = WLHNew([x.split(\" \") for x in dataDict[\"tweets\"]], dataDict[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "topWlh = [x.lower() for (x,y) in wlhCountRestricted.items() if y > 2.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentTestDataNew(data, topWlh, vocabulary, wlh):\n",
    "    new = []\n",
    "    wlhTerms = set(wlh.keys())\n",
    "    for i in range(0, len(data)):\n",
    "        tweet = data[i]\n",
    "        newTweet = copy(data[i])\n",
    "        for j in topWlh:\n",
    "            if j in tweet.lower():\n",
    "                newTweet += (\" \" + j)\n",
    "            for term in newTweet.split(\" \"):\n",
    "                if term in wlhTerms and wlh[term] > 2.5 and term in j:\n",
    "                    newTweet += (\" \" + j)\n",
    "        new.append(newTweet)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with dev set augmentation: 0.34459749169257153\n",
      "accuracy without dev set augmentation: 0.3444635009111373\n",
      "accuracy with new dev set augmentation: 0.36014042233894306\n"
     ]
    }
   ],
   "source": [
    "geotaggerNBMultinomial5 = Pipeline([('count', CountVectorizer()), ('classifier', MultinomialNB()),])\n",
    "geotaggerNBMultinomial5.fit(augmentTestDataNew(augmentedDataDict[\"tweets\"], topWlh, vocabulary, wlh), augmentedDataDict[\"labels\"])\n",
    "\n",
    "print(\"accuracy with dev set augmentation: {}\".format(geotaggerNBMultinomial5.score(augmentedDevDataDict[\"tweets\"], augmentedDevDataDict[\"labels\"])))\n",
    "print(\"accuracy without dev set augmentation: {}\".format(geotaggerNBMultinomial5.score(devDataDict[\"tweets\"], devDataDict[\"labels\"])))\n",
    "\n",
    "print(\"accuracy with new dev set augmentation: {}\".format(geotaggerNBMultinomial5.score(augmentTestData(augmentedDevDataDict[\"tweets\"], topWlh), devDataDict[\"labels\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with new new dev set augmentation: 0.32211383856790654\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy with new new dev set augmentation: {}\".format(geotaggerNBMultinomial5.score(augmentTestDataNew(augmentedDevDataDict[\"tweets\"], topWlh, vocabulary, wlh), devDataDict[\"labels\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
